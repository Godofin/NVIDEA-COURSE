A virtualização permite a continuidade dos negócios e o balanceamento de carga de trabalho. A capacidade de alocar recursos de GPU de forma flexível permite uma melhor utilização dos recursos do data center. Como a virtualização garante que todos os dados permaneçam com segurança no data center, essa solução ajuda a garantir a segurança da infraestrutura e dos dados.

Agora, vamos explorar o deep learning. Começaremos com uma breve revisão do que é, depois passaremos pelo fluxo de trabalho da IA. A partir daí, falaremos sobre a pilha de software de IA e o Cuda X. O deep learning é uma subcategoria do aprendizado de máquina. Ele utiliza redes neurais para treinar um modelo usando grandes conjuntos de dados, que podem atingir terabytes ou mais. Redes neurais são algoritmos que imitam o funcionamento do cérebro humano para compreender padrões complexos.

Os dados rotulados são aqueles que possuem etiquetas para ajudar a rede neural a aprender. No exemplo apresentado, as etiquetas são os objetos presentes nas imagens, como carros e caminhões. Os erros que o classificador comete durante o treinamento são usados para melhorar gradualmente a estrutura da rede. Quando o modelo baseado em rede neural é treinado, ele pode fazer previsões sobre novas imagens. Após o treinamento, a rede e o classificador são implementados para analisar dados que não foram rotulados previamente. Se o treinamento foi bem feito, a rede conseguirá aplicar sua representação de características para classificar corretamente categorias semelhantes em diferentes situações.

Para compreender o ecossistema de IA, é necessário entender o fluxo de trabalho. O primeiro passo é preparar os dados brutos e torná-los adequados para o modelo de aprendizado de máquina. Exemplos de ferramentas para isso incluem **Nvidia Rapids** e **Nvidia Rapids Accelerator para Apache Spark**. Depois que os dados são processados, passamos para a fase de treinamento, onde ensinamos o modelo a interpretar os dados. Algumas ferramentas utilizadas nessa fase incluem **PyTorch**, **Nvidia Tau Toolkit** e **TensorFlow**. Em seguida, refinamos os dados através da otimização, utilizando ferramentas como **TensorRT**. Finalmente, implementamos o modelo, tornando-o disponível para sistemas que recebem dados e retornam previsões. O **Nvidia Triton Inference Server** facilita a implementação escalável de modelos de IA em produção.

Os frameworks são conjuntos de ferramentas que permitem que cientistas de dados e especialistas desenvolvam modelos de IA de maneira mais acessível. Eles podem ser bibliotecas ou interfaces que ajudam desenvolvedores a construir modelos rapidamente. Cientistas de dados utilizam frameworks para criar modelos em diversas áreas, como visão computacional, processamento de linguagem natural (NLP) e reconhecimento de fala. Por exemplo, **MXNet** é um framework moderno de deep learning que permite treinar e implementar redes neurais profundas de maneira escalável, suportando múltiplas GPUs e diferentes linguagens de programação.

O **Scikit-Learn** é uma biblioteca gratuita de aprendizado de máquina para Python, que oferece algoritmos de classificação, regressão e clustering, além de integração com bibliotecas como **NumPy** e **SciPy**. O **TensorFlow** é uma biblioteca popular para aprendizado profundo e aprendizado de máquina. Já o **Nvidia Isaac Lab** é uma aplicação leve baseada no Isaac Sim, projetada para o aprendizado de robôs, otimizando técnicas de aprendizado por reforço, imitação e transferência.

O hardware necessário para deep learning inclui sistemas com uma ou mais GPUs, que utilizam drivers e sistemas operacionais compatíveis. O deep learning depende da computação acelerada por GPUs para oferecer alto desempenho. Atualmente, os containers são a opção preferida para desenvolvimento em organizações. A **Nvidia NGC (Nvidia GPU Cloud)** oferece diversos frameworks como containers Docker, permitindo desenvolvimento e implantação mais rápidos e portáveis de aplicações de IA. Esses containers são otimizados para rodar em **GPUs Nvidia** em ambientes de nuvem, data centers e dispositivos de borda.

O **CUDA Toolkit** é um modelo de programação paralela inovador da Nvidia que oferece otimizações essenciais para deep learning, aprendizado de máquina e computação de alto desempenho. Existem duas formas de construir uma plataforma de IA: utilizar uma abordagem **"faça você mesmo" (DIY)** ou utilizar o **Nvidia AI Enterprise**. O software de código aberto é amplamente utilizado na IA porque pode ser compartilhado e modificado colaborativamente. No entanto, construir uma plataforma de IA baseada apenas em código aberto pode ser arriscado, pois geralmente não há suporte robusto para produção. O **Nvidia AI Enterprise** permite que empresas utilizem práticas de código aberto para construir aplicações críticas de IA dentro da plataforma Nvidia, oferecendo suporte técnico e certificações para GPUs atuais e futuras.

O catálogo da **NGC** fornece software em containers para IA, HPC (computação de alto desempenho), ciência de dados e aplicações de visualização, simplificando e acelerando fluxos de trabalho. A NGC também oferece **modelos pré-treinados** para visão computacional, NLP e sistemas de recomendação. Esses modelos podem ser ajustados com dados específicos da empresa, reduzindo o tempo de desenvolvimento.

A NGC também oferece **Helm Charts** para facilitar a implantação de aplicações e coleções que reúnem todos os componentes necessários para criar aplicações de IA. Os modelos pré-treinados no catálogo NGC são desenvolvidos e continuamente aprimorados por especialistas da Nvidia. Para cada modelo, a Nvidia fornece detalhes sobre o conjunto de dados utilizado, número de épocas de treinamento, tamanho do lote e precisão alcançada.

Além disso, a Nvidia oferece SDKs como **Riva** para IA conversacional e **DeepStream** para análise de vídeo. Containers tornaram-se essenciais para desenvolvimento e implantação de software, garantindo que aplicações de IA sejam executadas de forma consistente em diferentes ambientes. Containers NGC são projetados para suportar múltiplas GPUs e ambientes Kubernetes, oferecendo suporte empresarial via **Nvidia AI Enterprise**.

O **Nvidia AI Enterprise** cobre toda a infraestrutura de IA, desde otimizações no hardware até ferramentas de gerenciamento e implantação na nuvem. Ele inclui software para desenvolvimento e implantação de IA, frameworks, workflows e modelos pré-treinados projetados para empresas.

A maioria das empresas está adotando IA, mas muitas enfrentam desafios na produção. Os **AI Workflows** da Nvidia são projetados para ajudar essas empresas a iniciar seus projetos de IA com exemplos de referência, frameworks, modelos pré-treinados, pipelines de treinamento e inferência, notebooks Jupyter e Helm Charts. Esses componentes são selecionados para acelerar o desenvolvimento e a entrega de soluções de IA.

Os benefícios incluem rápida implementação, alta precisão e suporte técnico da Nvidia. As empresas podem desenvolver e implantar soluções rapidamente e garantir desempenho e precisão otimizados. Para recapitular, agora você deve ser capaz de:

- Descrever o **VGPU** como base do ecossistema de IA.
- Explicar a pilha de software de **deep learning** da Nvidia e o ecossistema **Cuda X**.
- Definir as etapas de um **fluxo de trabalho de IA** e identificar ferramentas disponíveis para cada etapa.
- Explicar o que são frameworks e identificar frameworks **open source, de terceiros e da Nvidia**.
- Descrever os benefícios do **NGC** e do **Enterprise Catalog** na construção de soluções de IA.
- Explicar as vantagens do **Nvidia AI Enterprise** e seus casos de uso.
- Descrever os **AI Workflows** fornecidos pela Nvidia.
